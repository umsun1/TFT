# 🎮 TFT.GG - 대규모 데이터 파이프라인 및 랭킹 서비스 포트폴리오

> **"사용자에게 가치 있는 데이터를 가장 빠르고 안정적으로 전달하는 백엔드 개발자입니다."**
> 라이엇 게임즈 API의 엄격한 제한을 극복한 **안정적인 데이터 파이프라인** 구축 경험과, 조회 성능을 **15배 개선**한 경험을 통해 서비스의 완성도를 높였습니다.

---

## 📷 Service Preview (서비스 소개)

*(여기에 전적 조회 결과 화면 캡쳐 이미지를 넣어주세요)*

**TFT.GG**는 Riot Games API를 기반으로 TFT 전적 정보를 조회할 수 있는 서비스입니다.
사용자는 소환사 정보를 검색하여 최근 매치 전적과 주요 플레이 데이터를 확인할 수 있으며, 해당 화면은 서버에 저장된 전적 데이터를 기반으로 구성됩니다.

---

## 📌 Project Overview
*   **프로젝트명:** TFT.GG (전략적 팀 전투 전적 검색 서비스)
*   **개발 기간:** 2024.12 ~ 2026.01 (1인 개발)
*   **핵심 목표:** API 호출 제한(Rate Limit)이 엄격한 환경에서 **데이터 유실 0%**의 수집 시스템 구축 및 안정적인 전적 조회 서비스 제공.

---

## 💻 Key Implementation (핵심 구현 내용)

### 1. Web/Batch 분리 및 하이브리드 수집 아키텍처
> **"응답 속도 최적화와 안정적인 데이터 파이프라인의 조화"**

전적 조회 화면을 안정적으로 제공하기 위해, 실시간 API 호출의 부담을 줄이고 데이터 적재 효율을 높인 **Web/Batch 분리 아키텍처**를 설계했습니다.

*   **기존 방식의 문제점:** 사용자 요청 시마다 무거운 전적 리스트(Match 상세)를 실시간으로 호출하여, 외부 API 상태에 따라 페이지 로딩이 느려지거나 Rate Limit 도달 시 서비스가 마비되는 한계가 있었습니다.
*   **해결 전략 (Asynchronous Delegation):**
    *   **Web Server (Lightweight On-Demand):** 사용자가 검색 시 소환사의 존재 여부와 티어 정보 등 **최소한의 메타데이터만 실시간 API로 확인**하여 즉각적인 피드백을 제공합니다.
    *   **비동기 위임 (Priority Queue):** 시간이 오래 걸리는 매치 데이터 수집은 직접 수행하지 않고, **우선순위 큐(Priority 999)**에 작업을 등록하여 Batch 서버가 최우선적으로 처리하도록 위임합니다.
    *   **효과:** 전적 리스트는 오직 DB에서만 조회하여, 데이터 수집 중에도 사용자에게는 **0.1초 이내의 빠른 응답 속도**를 보장하는 구조를 완성했습니다.

### 2. Batch 서버 기반 매치 데이터 수집
> **"데이터 유실 없는 자동 복구 시스템"**

Riot Games API를 통해 매치 데이터를 안정적으로 수집하는 **전용 Batch 서버**를 구현했습니다.

*   **우선순위 기반 스케줄링:** Web 서버에서 등록한 긴급 작업(High Priority)을 일반적인 백그라운드 갱신 작업보다 먼저 처리하도록 설계하여 사용자 대기 시간을 최소화했습니다.
*   **상태 기반 복구 로직:** 수집 실패 시 `FAIL` 상태를 기록하고, 이후 재수집 과정에서 자동으로 `READY`로 복구(Self-healing)하는 로직을 통해 데이터 유실 0%를 달성했습니다.

### 3. 전적 데이터 저장 및 조회 구조 설계 (Hybrid Data Access)
> **"JPA의 생산성과 MyBatis의 조회 성능을 결합"**

매치, 참가자, 유닛, 시너지 정보를 관계형 DB 구조로 설계하여 효율적으로 저장하고 조회합니다.

*   **JPA (Write):** 복잡한 JSON 데이터를 관계형 DB에 적재할 때는 엔티티 간 연관 관계 관리가 용이한 **JPA**를 사용하여 개발 생산성을 높였습니다.
*   **MyBatis (Read):** 전적 조회 및 통계 데이터와 같이 복잡한 조인(Join)이 필요하고 성능이 중요한 조회 로직은 **MyBatis**로 직접 쿼리를 구성하여 최적화했습니다.

### 4. 전적 조회 화면 구성 (Frontend)
> **"사용자 경험을 고려한 데이터 시각화"**

*   **SSR 기반 렌더링:** Thymeleaf 기반의 서버 사이드 렌더링을 통해, 검색 엔진 최적화(SEO)와 빠른 초기 로딩 속도를 확보했습니다.
*   **데이터 시각화:** 전적 리스트 및 상세 정보를 뷰 템플릿에 바인딩하여, 텍스트 데이터가 아닌 직관적인 이미지(아이콘) 위주로 정보를 제공합니다.
*   **비동기 더보기 기능:** 초기 로딩 속도를 위해 일부 전적만 먼저 보여주고, 추가 전적은 **AJAX 비동기 요청**으로 불러오도록 구현하여 쾌적한 UX를 제공합니다.

---

## 🚀 Key Troubleshooting (핵심 문제 해결)

### 1️⃣ API Rate Limit 극복: "무조건 기다리는 게 답일까?"
**[상황]** 라이엇 API는 `20 requests every 1 seconds`와 같이 엄격한 속도 제한이 있습니다. 초기에는 단순 `Thread.sleep`으로 접근했으나, 네트워크 지연 등으로 인해 여전히 `429 Too Many Requests` 에러가 발생하고 수집기가 멈추는 현상이 발생했습니다.

**[해결: Smart Backoff 시스템 구축]**
`MatchFetchService`에 예외 처리 로직을 강화하여 **동적 대기 시스템**을 구현했습니다.
1.  `HttpClientErrorException.TooManyRequests` 발생 시 즉시 포착.
2.  헤더(`Retry-After`)에서 대기 시간(초)을 파싱.
3.  해당 시간만큼 대기 후, 작업 상태를 `FAIL`이 아닌 `READY`로 되돌려 **데이터 유실 없이 다음 사이클에 자동 재수집** 되도록 설계.

**결과:** 일일 약 5만 건의 매치 데이터를 **24시간 무중단**으로 수집하는 안정적인 파이프라인 완성.

<br>

### 2️⃣ 랭킹 조회 속도 15배 개선 (N+1 문제 해결)
**[상황]** 상위 100명의 랭커를 보여주는 페이지 진입 시, 로딩에 **약 1.2초**가 소요되었습니다. 로그를 분석해 보니 랭커 100명의 프로필을 가져오기 위해 **100번의 추가 쿼리**가 발생하고 있었습니다.

**[해결: In-Memory Mapping 최적화]**
1.  **Bulk Fetch:** `IN` 절을 사용하여 100명의 ID에 해당하는 프로필 데이터를 **한 번의 쿼리**로 조회.
2.  **Map 변환:** 조회된 List 데이터를 `Map<String, Dto>` 형태로 변환하여, 애플리케이션 메모리 상에서 O(1) 속도로 매칭.

**결과:** 쿼리 발생 수를 **101회 → 2회(98% 감소)**로 줄이고, 응답 속도를 **0.08초(15배 향상)**로 단축했습니다.

<br>

### 3️⃣ Batch 서버 수집 성능 20배 향상 (Bulk Operation 적용)
**[상황]** 매치 수집 스케줄러가 소환사 1명(약 500게임 기준)을 처리하는 데 **평균 10초 이상** 소요되는 심각한 병목 현상을 발견했습니다. 데이터가 쌓일수록 전체 수집 사이클이 지연되는 문제가 있었습니다.

**[분석 및 해결]**
*   **원인:** 반복문 내부에서 각 매치 ID마다 DB 존재 여부를 확인하는 `existsByGaId` 등의 쿼리가 매번 실행되어, 소환사 1명당 **약 1,000회 이상의 DB I/O**가 발생하고 있었습니다.
*   **해결:** 단건 쿼리를 **일괄 처리(Bulk Operation)** 방식으로 전환했습니다.
    1.  **Bulk Select:** `IN` 절을 사용하여 수백 개의 매치 ID 존재 여부를 **단 2회의 쿼리**로 한 번에 조회.
    2.  **In-Memory Filtering:** 조회 결과를 Java `Set`과 `Map`에 적재하여 메모리 내에서 중복 여부를 비교 연산.
    3.  **Bulk Insert:** 개별적으로 호출하던 `save()` 대신 `saveAll()`을 사용하여 **Batch Insert**로 저장 성능 최적화.

**[결과]**
| 구분 | 개선 전 (Before) | 개선 후 (After) | 개선율 |
|:---:|:---:|:---:|:---:|
| **DB 접근 횟수** | 약 1,000회 | **3회** | **99.7% 감소** |
| **처리 속도** | 평균 10.2초 | **평균 0.5초** | **20배 향상** |

👉 **성과:** 시스템 리소스 사용량을 획기적으로 낮추고, 대규모 데이터 파이프라인의 수집 처리량을 극대화했습니다.
